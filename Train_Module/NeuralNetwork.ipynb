{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252123, 91)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>S0</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>S7</th>\n",
       "      <th>S8</th>\n",
       "      <th>...</th>\n",
       "      <th>S80</th>\n",
       "      <th>S81</th>\n",
       "      <th>S82</th>\n",
       "      <th>S83</th>\n",
       "      <th>S84</th>\n",
       "      <th>S85</th>\n",
       "      <th>S86</th>\n",
       "      <th>S87</th>\n",
       "      <th>S88</th>\n",
       "      <th>S89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007</td>\n",
       "      <td>44.76752</td>\n",
       "      <td>114.82099</td>\n",
       "      <td>3.83239</td>\n",
       "      <td>27.99928</td>\n",
       "      <td>1.49153</td>\n",
       "      <td>-15.90853</td>\n",
       "      <td>28.24844</td>\n",
       "      <td>3.61650</td>\n",
       "      <td>-7.24653</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.89619</td>\n",
       "      <td>-471.02844</td>\n",
       "      <td>411.56205</td>\n",
       "      <td>443.01198</td>\n",
       "      <td>19.30254</td>\n",
       "      <td>309.07806</td>\n",
       "      <td>-336.91706</td>\n",
       "      <td>-14.70547</td>\n",
       "      <td>-474.44157</td>\n",
       "      <td>31.32820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004</td>\n",
       "      <td>52.28942</td>\n",
       "      <td>75.73319</td>\n",
       "      <td>11.35941</td>\n",
       "      <td>-6.20582</td>\n",
       "      <td>-27.64559</td>\n",
       "      <td>-30.75995</td>\n",
       "      <td>12.50955</td>\n",
       "      <td>7.47877</td>\n",
       "      <td>9.88498</td>\n",
       "      <td>...</td>\n",
       "      <td>4.57060</td>\n",
       "      <td>1.36110</td>\n",
       "      <td>-6.52977</td>\n",
       "      <td>59.48672</td>\n",
       "      <td>3.69790</td>\n",
       "      <td>-36.92252</td>\n",
       "      <td>44.08077</td>\n",
       "      <td>3.39993</td>\n",
       "      <td>-70.07591</td>\n",
       "      <td>3.86143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005</td>\n",
       "      <td>33.81773</td>\n",
       "      <td>-139.07371</td>\n",
       "      <td>134.19332</td>\n",
       "      <td>17.85216</td>\n",
       "      <td>63.47408</td>\n",
       "      <td>-25.28005</td>\n",
       "      <td>-34.65911</td>\n",
       "      <td>-5.99135</td>\n",
       "      <td>1.27848</td>\n",
       "      <td>...</td>\n",
       "      <td>54.16608</td>\n",
       "      <td>15.04530</td>\n",
       "      <td>39.09107</td>\n",
       "      <td>39.03041</td>\n",
       "      <td>3.68708</td>\n",
       "      <td>-61.88547</td>\n",
       "      <td>45.68115</td>\n",
       "      <td>6.39822</td>\n",
       "      <td>3.24471</td>\n",
       "      <td>35.74749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1998</td>\n",
       "      <td>41.60866</td>\n",
       "      <td>3.17811</td>\n",
       "      <td>-3.97174</td>\n",
       "      <td>23.53564</td>\n",
       "      <td>-19.68553</td>\n",
       "      <td>20.74407</td>\n",
       "      <td>18.80866</td>\n",
       "      <td>6.24474</td>\n",
       "      <td>-7.98424</td>\n",
       "      <td>...</td>\n",
       "      <td>28.08591</td>\n",
       "      <td>295.88684</td>\n",
       "      <td>54.02395</td>\n",
       "      <td>102.02880</td>\n",
       "      <td>40.47711</td>\n",
       "      <td>15.10258</td>\n",
       "      <td>-250.32293</td>\n",
       "      <td>2.81288</td>\n",
       "      <td>56.05172</td>\n",
       "      <td>3.60432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1987</td>\n",
       "      <td>44.49525</td>\n",
       "      <td>-32.25270</td>\n",
       "      <td>58.08217</td>\n",
       "      <td>3.73684</td>\n",
       "      <td>-32.53274</td>\n",
       "      <td>-18.72885</td>\n",
       "      <td>-15.85665</td>\n",
       "      <td>-3.34607</td>\n",
       "      <td>22.63786</td>\n",
       "      <td>...</td>\n",
       "      <td>31.44988</td>\n",
       "      <td>-136.50457</td>\n",
       "      <td>-85.11989</td>\n",
       "      <td>-74.96342</td>\n",
       "      <td>9.56921</td>\n",
       "      <td>-100.61689</td>\n",
       "      <td>-133.29315</td>\n",
       "      <td>9.19246</td>\n",
       "      <td>-97.37953</td>\n",
       "      <td>30.11015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year        S0         S1         S2        S3        S4        S5  \\\n",
       "0  2007  44.76752  114.82099    3.83239  27.99928   1.49153 -15.90853   \n",
       "1  2004  52.28942   75.73319   11.35941  -6.20582 -27.64559 -30.75995   \n",
       "2  2005  33.81773 -139.07371  134.19332  17.85216  63.47408 -25.28005   \n",
       "3  1998  41.60866    3.17811   -3.97174  23.53564 -19.68553  20.74407   \n",
       "4  1987  44.49525  -32.25270   58.08217   3.73684 -32.53274 -18.72885   \n",
       "\n",
       "         S6       S7        S8  ...       S80        S81        S82  \\\n",
       "0  28.24844  3.61650  -7.24653  ...  -1.89619 -471.02844  411.56205   \n",
       "1  12.50955  7.47877   9.88498  ...   4.57060    1.36110   -6.52977   \n",
       "2 -34.65911 -5.99135   1.27848  ...  54.16608   15.04530   39.09107   \n",
       "3  18.80866  6.24474  -7.98424  ...  28.08591  295.88684   54.02395   \n",
       "4 -15.85665 -3.34607  22.63786  ...  31.44988 -136.50457  -85.11989   \n",
       "\n",
       "         S83       S84        S85        S86       S87        S88       S89  \n",
       "0  443.01198  19.30254  309.07806 -336.91706 -14.70547 -474.44157  31.32820  \n",
       "1   59.48672   3.69790  -36.92252   44.08077   3.39993  -70.07591   3.86143  \n",
       "2   39.03041   3.68708  -61.88547   45.68115   6.39822    3.24471  35.74749  \n",
       "3  102.02880  40.47711   15.10258 -250.32293   2.81288   56.05172   3.60432  \n",
       "4  -74.96342   9.56921 -100.61689 -133.29315   9.19246  -97.37953  30.11015  \n",
       "\n",
       "[5 rows x 91 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train set:  181528\n",
      "Number of validation set:  20170\n",
      "Numebr of test set:  50425\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Year', axis=1)\n",
    "y = df['Year']\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Number of train set: \", X_train.shape[0])\n",
    "print(\"Number of validation set: \", X_val.shape[0])\n",
    "print(\"Numebr of test set: \", X_test.shape[0])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5673/5673 - 1s - loss: 63.8900 - 1s/epoch - 185us/step\n",
      "631/631 - 0s - loss: 69.6590 - 133ms/epoch - 210us/step\n",
      "5673/5673 - 1s - loss: 66.7849 - 1s/epoch - 186us/step\n",
      "631/631 - 0s - loss: 70.0763 - 128ms/epoch - 203us/step\n",
      "5673/5673 - 1s - loss: 65.5250 - 1s/epoch - 185us/step\n",
      "631/631 - 0s - loss: 69.7306 - 129ms/epoch - 204us/step\n",
      "5673/5673 - 1s - loss: 64.2362 - 1s/epoch - 188us/step\n",
      "631/631 - 0s - loss: 69.8102 - 129ms/epoch - 204us/step\n",
      "5673/5673 - 1s - loss: 67.0661 - 1s/epoch - 186us/step\n",
      "631/631 - 0s - loss: 70.3432 - 128ms/epoch - 202us/step\n",
      "5673/5673 - 1s - loss: 65.3082 - 1s/epoch - 188us/step\n",
      "631/631 - 0s - loss: 69.9076 - 128ms/epoch - 204us/step\n",
      "5673/5673 - 1s - loss: 68.3250 - 1s/epoch - 185us/step\n",
      "631/631 - 0s - loss: 71.5267 - 132ms/epoch - 209us/step\n",
      "5673/5673 - 1s - loss: 68.5359 - 1s/epoch - 185us/step\n",
      "631/631 - 0s - loss: 71.6644 - 127ms/epoch - 202us/step\n",
      "5673/5673 - 1s - loss: 68.8370 - 1s/epoch - 184us/step\n",
      "631/631 - 0s - loss: 72.0904 - 127ms/epoch - 201us/step\n",
      "5673/5673 - 1s - loss: 65.7908 - 1s/epoch - 196us/step\n",
      "631/631 - 0s - loss: 69.8020 - 135ms/epoch - 214us/step\n",
      "5673/5673 - 1s - loss: 64.3668 - 1s/epoch - 202us/step\n",
      "631/631 - 0s - loss: 69.4887 - 140ms/epoch - 223us/step\n",
      "5673/5673 - 1s - loss: 64.0147 - 1s/epoch - 201us/step\n",
      "631/631 - 0s - loss: 68.9602 - 139ms/epoch - 220us/step\n",
      "5673/5673 - 1s - loss: 66.5325 - 1s/epoch - 197us/step\n",
      "631/631 - 0s - loss: 70.3678 - 136ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 66.6625 - 1s/epoch - 196us/step\n",
      "631/631 - 0s - loss: 70.6939 - 135ms/epoch - 214us/step\n",
      "5673/5673 - 1s - loss: 70.0372 - 1s/epoch - 197us/step\n",
      "631/631 - 0s - loss: 72.4497 - 135ms/epoch - 213us/step\n",
      "5673/5673 - 1s - loss: 67.4720 - 1s/epoch - 195us/step\n",
      "631/631 - 0s - loss: 70.7996 - 133ms/epoch - 211us/step\n",
      "5673/5673 - 1s - loss: 69.7013 - 1s/epoch - 196us/step\n",
      "631/631 - 0s - loss: 72.4995 - 134ms/epoch - 212us/step\n",
      "5673/5673 - 1s - loss: 69.5795 - 1s/epoch - 194us/step\n",
      "631/631 - 0s - loss: 72.1004 - 136ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 65.3756 - 1s/epoch - 200us/step\n",
      "631/631 - 0s - loss: 69.5577 - 137ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 65.8750 - 1s/epoch - 200us/step\n",
      "631/631 - 0s - loss: 69.6739 - 136ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 77.3274 - 1s/epoch - 199us/step\n",
      "631/631 - 0s - loss: 80.7089 - 136ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 66.2479 - 1s/epoch - 200us/step\n",
      "631/631 - 0s - loss: 69.9361 - 136ms/epoch - 216us/step\n",
      "5673/5673 - 1s - loss: 68.1252 - 1s/epoch - 198us/step\n",
      "631/631 - 0s - loss: 70.4923 - 135ms/epoch - 213us/step\n",
      "5673/5673 - 1s - loss: 72.6488 - 1s/epoch - 197us/step\n",
      "631/631 - 0s - loss: 74.6099 - 142ms/epoch - 224us/step\n",
      "5673/5673 - 1s - loss: 69.9811 - 1s/epoch - 203us/step\n",
      "631/631 - 0s - loss: 71.7215 - 138ms/epoch - 219us/step\n",
      "5673/5673 - 1s - loss: 69.0679 - 1s/epoch - 203us/step\n",
      "631/631 - 0s - loss: 71.5893 - 137ms/epoch - 217us/step\n",
      "5673/5673 - 1s - loss: 79.8585 - 1s/epoch - 207us/step\n",
      "631/631 - 0s - loss: 81.7835 - 142ms/epoch - 225us/step\n",
      "5673/5673 - 1s - loss: 59.8500 - 1s/epoch - 208us/step\n",
      "631/631 - 0s - loss: 68.6101 - 142ms/epoch - 225us/step\n",
      "5673/5673 - 1s - loss: 59.2936 - 1s/epoch - 210us/step\n",
      "631/631 - 0s - loss: 68.2286 - 144ms/epoch - 228us/step\n",
      "5673/5673 - 1s - loss: 58.5386 - 1s/epoch - 208us/step\n",
      "631/631 - 0s - loss: 68.5241 - 142ms/epoch - 225us/step\n",
      "5673/5673 - 1s - loss: 60.5992 - 1s/epoch - 206us/step\n",
      "631/631 - 0s - loss: 68.6988 - 140ms/epoch - 221us/step\n",
      "5673/5673 - 1s - loss: 62.6753 - 1s/epoch - 205us/step\n",
      "631/631 - 0s - loss: 68.6625 - 140ms/epoch - 222us/step\n",
      "5673/5673 - 1s - loss: 61.3686 - 1s/epoch - 207us/step\n",
      "631/631 - 0s - loss: 68.4201 - 140ms/epoch - 222us/step\n",
      "5673/5673 - 1s - loss: 62.2450 - 1s/epoch - 208us/step\n",
      "631/631 - 0s - loss: 68.8361 - 143ms/epoch - 227us/step\n",
      "5673/5673 - 1s - loss: 62.6507 - 1s/epoch - 208us/step\n",
      "631/631 - 0s - loss: 69.1210 - 142ms/epoch - 226us/step\n",
      "5673/5673 - 1s - loss: 62.1071 - 1s/epoch - 208us/step\n",
      "631/631 - 0s - loss: 68.7288 - 160ms/epoch - 254us/step\n",
      "5673/5673 - 1s - loss: 57.3041 - 1s/epoch - 229us/step\n",
      "631/631 - 0s - loss: 68.3625 - 155ms/epoch - 246us/step\n",
      "5673/5673 - 1s - loss: 57.6744 - 1s/epoch - 224us/step\n",
      "631/631 - 0s - loss: 68.3393 - 151ms/epoch - 240us/step\n",
      "5673/5673 - 1s - loss: 59.0479 - 1s/epoch - 224us/step\n",
      "631/631 - 0s - loss: 68.1973 - 151ms/epoch - 239us/step\n",
      "5673/5673 - 1s - loss: 61.4401 - 1s/epoch - 226us/step\n",
      "631/631 - 0s - loss: 68.9071 - 153ms/epoch - 242us/step\n",
      "5673/5673 - 1s - loss: 60.6296 - 1s/epoch - 224us/step\n",
      "631/631 - 0s - loss: 68.5196 - 152ms/epoch - 241us/step\n",
      "5673/5673 - 1s - loss: 60.2912 - 1s/epoch - 225us/step\n",
      "631/631 - 0s - loss: 67.7349 - 153ms/epoch - 242us/step\n",
      "5673/5673 - 1s - loss: 64.2855 - 1s/epoch - 225us/step\n",
      "631/631 - 0s - loss: 69.9618 - 153ms/epoch - 242us/step\n",
      "5673/5673 - 1s - loss: 65.2266 - 1s/epoch - 227us/step\n",
      "631/631 - 0s - loss: 69.8755 - 152ms/epoch - 241us/step\n",
      "5673/5673 - 1s - loss: 64.7710 - 1s/epoch - 226us/step\n",
      "631/631 - 0s - loss: 69.4785 - 154ms/epoch - 243us/step\n",
      "5673/5673 - 1s - loss: 63.7453 - 1s/epoch - 231us/step\n",
      "631/631 - 0s - loss: 69.0925 - 156ms/epoch - 248us/step\n",
      "5673/5673 - 1s - loss: 60.6896 - 1s/epoch - 236us/step\n",
      "631/631 - 0s - loss: 68.7584 - 158ms/epoch - 251us/step\n",
      "5673/5673 - 1s - loss: 58.5734 - 1s/epoch - 241us/step\n",
      "631/631 - 0s - loss: 68.7710 - 161ms/epoch - 255us/step\n",
      "5673/5673 - 1s - loss: 66.1259 - 1s/epoch - 239us/step\n",
      "631/631 - 0s - loss: 70.0208 - 161ms/epoch - 255us/step\n",
      "5673/5673 - 1s - loss: 62.4416 - 1s/epoch - 238us/step\n",
      "631/631 - 0s - loss: 69.0622 - 160ms/epoch - 254us/step\n",
      "5673/5673 - 1s - loss: 61.2901 - 1s/epoch - 239us/step\n",
      "631/631 - 0s - loss: 69.0464 - 160ms/epoch - 253us/step\n",
      "5673/5673 - 1s - loss: 66.0678 - 1s/epoch - 233us/step\n",
      "631/631 - 0s - loss: 70.2007 - 157ms/epoch - 249us/step\n",
      "5673/5673 - 1s - loss: 64.4762 - 1s/epoch - 237us/step\n",
      "631/631 - 0s - loss: 69.4705 - 158ms/epoch - 250us/step\n",
      "5673/5673 - 1s - loss: 64.9873 - 1s/epoch - 238us/step\n",
      "631/631 - 0s - loss: 69.4498 - 160ms/epoch - 254us/step\n",
      "5673/5673 - 1s - loss: 55.2402 - 1s/epoch - 250us/step\n",
      "631/631 - 0s - loss: 67.9394 - 166ms/epoch - 264us/step\n",
      "5673/5673 - 2s - loss: 54.9785 - 2s/epoch - 277us/step\n",
      "631/631 - 0s - loss: 68.0672 - 185ms/epoch - 294us/step\n",
      "5673/5673 - 1s - loss: 56.2621 - 1s/epoch - 253us/step\n",
      "631/631 - 0s - loss: 68.5178 - 171ms/epoch - 271us/step\n",
      "5673/5673 - 1s - loss: 57.6876 - 1s/epoch - 248us/step\n",
      "631/631 - 0s - loss: 68.2308 - 171ms/epoch - 271us/step\n",
      "5673/5673 - 1s - loss: 50.5644 - 1s/epoch - 249us/step\n",
      "631/631 - 0s - loss: 67.5767 - 165ms/epoch - 262us/step\n",
      "5673/5673 - 1s - loss: 54.4598 - 1s/epoch - 248us/step\n",
      "631/631 - 0s - loss: 68.0750 - 167ms/epoch - 264us/step\n",
      "5673/5673 - 1s - loss: 61.0675 - 1s/epoch - 244us/step\n",
      "631/631 - 0s - loss: 67.8155 - 163ms/epoch - 259us/step\n",
      "5673/5673 - 1s - loss: 58.4058 - 1s/epoch - 247us/step\n",
      "631/631 - 0s - loss: 67.8272 - 170ms/epoch - 269us/step\n",
      "5673/5673 - 1s - loss: 57.7106 - 1s/epoch - 242us/step\n",
      "631/631 - 0s - loss: 67.9596 - 160ms/epoch - 254us/step\n",
      "5673/5673 - 2s - loss: 52.7253 - 2s/epoch - 302us/step\n",
      "631/631 - 0s - loss: 68.3676 - 205ms/epoch - 325us/step\n",
      "5673/5673 - 2s - loss: 54.0245 - 2s/epoch - 326us/step\n",
      "631/631 - 0s - loss: 68.5651 - 213ms/epoch - 338us/step\n",
      "5673/5673 - 2s - loss: 54.2984 - 2s/epoch - 318us/step\n",
      "631/631 - 0s - loss: 68.5059 - 207ms/epoch - 328us/step\n",
      "5673/5673 - 2s - loss: 58.4009 - 2s/epoch - 307us/step\n",
      "631/631 - 0s - loss: 69.0322 - 205ms/epoch - 324us/step\n",
      "5673/5673 - 2s - loss: 59.7680 - 2s/epoch - 310us/step\n",
      "631/631 - 0s - loss: 68.5330 - 210ms/epoch - 332us/step\n",
      "5673/5673 - 2s - loss: 63.2628 - 2s/epoch - 322us/step\n",
      "631/631 - 0s - loss: 68.6632 - 212ms/epoch - 337us/step\n",
      "5673/5673 - 2s - loss: 63.6155 - 2s/epoch - 303us/step\n",
      "631/631 - 0s - loss: 69.7164 - 198ms/epoch - 314us/step\n",
      "5673/5673 - 2s - loss: 63.2064 - 2s/epoch - 312us/step\n",
      "631/631 - 0s - loss: 69.4987 - 208ms/epoch - 329us/step\n",
      "5673/5673 - 2s - loss: 59.8649 - 2s/epoch - 315us/step\n",
      "631/631 - 0s - loss: 68.7338 - 209ms/epoch - 331us/step\n",
      "5673/5673 - 2s - loss: 61.2371 - 2s/epoch - 375us/step\n",
      "631/631 - 0s - loss: 69.0105 - 250ms/epoch - 396us/step\n",
      "5673/5673 - 2s - loss: 59.3311 - 2s/epoch - 349us/step\n",
      "631/631 - 0s - loss: 68.9451 - 229ms/epoch - 363us/step\n",
      "5673/5673 - 2s - loss: 60.6602 - 2s/epoch - 353us/step\n",
      "631/631 - 0s - loss: 68.5408 - 223ms/epoch - 354us/step\n",
      "5673/5673 - 2s - loss: 66.1196 - 2s/epoch - 344us/step\n",
      "631/631 - 0s - loss: 69.9138 - 224ms/epoch - 355us/step\n",
      "5673/5673 - 2s - loss: 60.4978 - 2s/epoch - 339us/step\n",
      "631/631 - 0s - loss: 68.8011 - 223ms/epoch - 354us/step\n",
      "5673/5673 - 2s - loss: 55.9322 - 2s/epoch - 367us/step\n",
      "631/631 - 0s - loss: 68.7194 - 242ms/epoch - 383us/step\n",
      "5673/5673 - 2s - loss: 65.9647 - 2s/epoch - 360us/step\n",
      "631/631 - 0s - loss: 70.3329 - 238ms/epoch - 377us/step\n",
      "5673/5673 - 2s - loss: 63.6892 - 2s/epoch - 362us/step\n",
      "631/631 - 0s - loss: 69.2530 - 244ms/epoch - 387us/step\n",
      "5673/5673 - 2s - loss: 60.9176 - 2s/epoch - 341us/step\n",
      "631/631 - 0s - loss: 69.0041 - 228ms/epoch - 362us/step\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 63.88999557495117, 'val_loss': 69.65901184082031}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 66.78487396240234, 'val_loss': 70.07627868652344}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 65.5250473022461, 'val_loss': 69.73062896728516}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 64.23619842529297, 'val_loss': 69.81019592285156}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 67.06614685058594, 'val_loss': 70.34324645996094}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 65.30815887451172, 'val_loss': 69.90755462646484}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 68.3249740600586, 'val_loss': 71.52667236328125}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 68.535888671875, 'val_loss': 71.66439056396484}\n",
      "{'hidden_size': 64, 'depth': 2, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 68.83695220947266, 'val_loss': 72.0903549194336}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 65.79083251953125, 'val_loss': 69.802001953125}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 64.36675262451172, 'val_loss': 69.48873901367188}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 64.01470947265625, 'val_loss': 68.960205078125}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 66.53253936767578, 'val_loss': 70.36783599853516}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 66.66248321533203, 'val_loss': 70.69390869140625}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 70.03724670410156, 'val_loss': 72.44974517822266}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 67.47196197509766, 'val_loss': 70.79955291748047}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 69.70128631591797, 'val_loss': 72.49952697753906}\n",
      "{'hidden_size': 64, 'depth': 3, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 69.57950592041016, 'val_loss': 72.10042572021484}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 65.3756103515625, 'val_loss': 69.55766296386719}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 65.87503814697266, 'val_loss': 69.67388153076172}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 77.32736206054688, 'val_loss': 80.70894622802734}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 66.24787139892578, 'val_loss': 69.93612670898438}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 68.1252212524414, 'val_loss': 70.4922866821289}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 72.64877319335938, 'val_loss': 74.6098861694336}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 69.98109436035156, 'val_loss': 71.72147369384766}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 69.06790161132812, 'val_loss': 71.58934020996094}\n",
      "{'hidden_size': 64, 'depth': 4, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 79.85845947265625, 'val_loss': 81.78350830078125}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 59.849979400634766, 'val_loss': 68.61009979248047}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 59.2935791015625, 'val_loss': 68.22859954833984}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 58.53864288330078, 'val_loss': 68.52407836914062}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 60.59922409057617, 'val_loss': 68.69880676269531}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 62.675315856933594, 'val_loss': 68.66252136230469}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 61.36856460571289, 'val_loss': 68.42005920410156}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 62.24498748779297, 'val_loss': 68.83611297607422}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 62.650726318359375, 'val_loss': 69.1209945678711}\n",
      "{'hidden_size': 128, 'depth': 2, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 62.10706329345703, 'val_loss': 68.72884368896484}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 57.30408477783203, 'val_loss': 68.36253356933594}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 57.674354553222656, 'val_loss': 68.33929443359375}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 59.04789733886719, 'val_loss': 68.19727325439453}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 61.44009017944336, 'val_loss': 68.90709686279297}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 60.62958908081055, 'val_loss': 68.51957702636719}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 60.2912483215332, 'val_loss': 67.73491668701172}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 64.2855224609375, 'val_loss': 69.9618148803711}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 65.22660064697266, 'val_loss': 69.8755111694336}\n",
      "{'hidden_size': 128, 'depth': 3, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 64.77096557617188, 'val_loss': 69.47845458984375}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 63.74531936645508, 'val_loss': 69.0925064086914}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 60.68963623046875, 'val_loss': 68.75836944580078}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 58.57339859008789, 'val_loss': 68.77098083496094}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 66.12589263916016, 'val_loss': 70.02079010009766}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 62.441612243652344, 'val_loss': 69.06217193603516}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 61.29011917114258, 'val_loss': 69.04640197753906}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 66.06782531738281, 'val_loss': 70.20072174072266}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 64.47624969482422, 'val_loss': 69.47051239013672}\n",
      "{'hidden_size': 128, 'depth': 4, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 64.98729705810547, 'val_loss': 69.44977569580078}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 55.24022674560547, 'val_loss': 67.93943786621094}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 54.978515625, 'val_loss': 68.06722259521484}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 56.262115478515625, 'val_loss': 68.51776885986328}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 57.68757247924805, 'val_loss': 68.23081970214844}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 50.56438064575195, 'val_loss': 67.57671356201172}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 54.45982360839844, 'val_loss': 68.07496643066406}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 61.067466735839844, 'val_loss': 67.81551361083984}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 58.40577697753906, 'val_loss': 67.82717895507812}\n",
      "{'hidden_size': 256, 'depth': 2, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 57.710594177246094, 'val_loss': 67.95964050292969}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 52.72531509399414, 'val_loss': 68.36764526367188}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 54.024532318115234, 'val_loss': 68.56513214111328}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 54.29837417602539, 'val_loss': 68.50588989257812}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 58.400917053222656, 'val_loss': 69.0322036743164}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 59.76796340942383, 'val_loss': 68.53296661376953}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 63.26279067993164, 'val_loss': 68.66316223144531}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 63.615535736083984, 'val_loss': 69.71644592285156}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 63.206398010253906, 'val_loss': 69.49869537353516}\n",
      "{'hidden_size': 256, 'depth': 3, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 59.86494064331055, 'val_loss': 68.73383331298828}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.3, 'batch_size': 32, 'train_loss': 61.2370719909668, 'val_loss': 69.01051330566406}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.3, 'batch_size': 64, 'train_loss': 59.33111572265625, 'val_loss': 68.94506072998047}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.3, 'batch_size': 128, 'train_loss': 60.66019058227539, 'val_loss': 68.54081726074219}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.4, 'batch_size': 32, 'train_loss': 66.11962890625, 'val_loss': 69.91380310058594}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 60.497779846191406, 'val_loss': 68.80109405517578}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.4, 'batch_size': 128, 'train_loss': 55.932220458984375, 'val_loss': 68.7193832397461}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.5, 'batch_size': 32, 'train_loss': 65.96472930908203, 'val_loss': 70.33294677734375}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.5, 'batch_size': 64, 'train_loss': 63.689151763916016, 'val_loss': 69.25300598144531}\n",
      "{'hidden_size': 256, 'depth': 4, 'dropout': 0.5, 'batch_size': 128, 'train_loss': 60.91763687133789, 'val_loss': 69.00411987304688}\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [64, 128, 256]  # Dimensioni dei layer nascosti\n",
    "depths = [2, 3, 4]  # Numero di layer nascosti\n",
    "dropouts = [0.3, 0.4, 0.5]  # Dropout \n",
    "batch_sizes = [32, 64, 128]  # Dimensioni del batch\n",
    "\n",
    "results = []\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for depth in depths:\n",
    "        for dropout in dropouts:\n",
    "            for batch_size in batch_sizes:\n",
    "                model = Sequential()\n",
    "                model.add(Dense(hidden_size, input_dim=X_train.shape[1], activation='relu'))\n",
    "                for _ in range(depth - 1):  \n",
    "                    model.add(Dropout(dropout))\n",
    "                    model.add(Dense(hidden_size, activation='relu'))\n",
    "                model.add(Dense(1)) \n",
    "                \n",
    "                learning_rate = 0.001  \n",
    "                \n",
    "                optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "                model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "                \n",
    "                history = model.fit(\n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping, reduce_lr],\n",
    "                    verbose=0  \n",
    "                )\n",
    "                \n",
    "                train_loss = model.evaluate(X_train, y_train, verbose=2)\n",
    "                val_loss = model.evaluate(X_val, y_val, verbose=2)\n",
    "                \n",
    "                results.append({\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'depth': depth,\n",
    "                    'dropout': dropout,\n",
    "                    'batch_size': batch_size,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss\n",
    "                })\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miglior configurazione: {'hidden_size': 256, 'depth': 2, 'dropout': 0.4, 'batch_size': 64, 'train_loss': 50.56438064575195, 'val_loss': 67.57671356201172}\n"
     ]
    }
   ],
   "source": [
    "best_result = min(results, key=lambda x: x['val_loss'])\n",
    "\n",
    "print(f\"Miglior configurazione: {best_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I migliori risultati si ottengono con la seguente rete neurale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               23296     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89,345\n",
      "Trainable params: 89,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "2837/2837 [==============================] - 3s 865us/step - loss: 123.2446 - val_loss: 82.4552 - lr: 0.0010\n",
      "Epoch 2/200\n",
      "2837/2837 [==============================] - 2s 822us/step - loss: 84.4551 - val_loss: 77.4652 - lr: 0.0010\n",
      "Epoch 3/200\n",
      "2837/2837 [==============================] - 2s 845us/step - loss: 78.5165 - val_loss: 73.3518 - lr: 0.0010\n",
      "Epoch 4/200\n",
      "2837/2837 [==============================] - 2s 825us/step - loss: 75.9905 - val_loss: 71.5497 - lr: 0.0010\n",
      "Epoch 5/200\n",
      "2837/2837 [==============================] - 2s 829us/step - loss: 74.5770 - val_loss: 71.8325 - lr: 0.0010\n",
      "Epoch 6/200\n",
      "2837/2837 [==============================] - 2s 838us/step - loss: 73.6002 - val_loss: 71.8754 - lr: 0.0010\n",
      "Epoch 7/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 72.8146 - val_loss: 71.2356 - lr: 0.0010\n",
      "Epoch 8/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 72.1407 - val_loss: 71.2946 - lr: 0.0010\n",
      "Epoch 9/200\n",
      "2837/2837 [==============================] - 2s 833us/step - loss: 71.4772 - val_loss: 70.4060 - lr: 0.0010\n",
      "Epoch 10/200\n",
      "2837/2837 [==============================] - 2s 830us/step - loss: 70.8004 - val_loss: 70.8965 - lr: 0.0010\n",
      "Epoch 11/200\n",
      "2837/2837 [==============================] - 2s 835us/step - loss: 70.3201 - val_loss: 69.8488 - lr: 0.0010\n",
      "Epoch 12/200\n",
      "2837/2837 [==============================] - 2s 868us/step - loss: 69.8373 - val_loss: 71.1257 - lr: 0.0010\n",
      "Epoch 13/200\n",
      "2837/2837 [==============================] - 2s 862us/step - loss: 69.5689 - val_loss: 69.3695 - lr: 0.0010\n",
      "Epoch 14/200\n",
      "2837/2837 [==============================] - 2s 815us/step - loss: 69.1169 - val_loss: 69.2775 - lr: 0.0010\n",
      "Epoch 15/200\n",
      "2837/2837 [==============================] - 2s 823us/step - loss: 68.8390 - val_loss: 68.8935 - lr: 0.0010\n",
      "Epoch 16/200\n",
      "2837/2837 [==============================] - 2s 814us/step - loss: 68.3341 - val_loss: 69.8423 - lr: 0.0010\n",
      "Epoch 17/200\n",
      "2837/2837 [==============================] - 2s 806us/step - loss: 68.0121 - val_loss: 69.7305 - lr: 0.0010\n",
      "Epoch 18/200\n",
      "2837/2837 [==============================] - 2s 809us/step - loss: 67.6514 - val_loss: 68.8989 - lr: 0.0010\n",
      "Epoch 19/200\n",
      "2837/2837 [==============================] - 2s 832us/step - loss: 67.3708 - val_loss: 69.2791 - lr: 0.0010\n",
      "Epoch 20/200\n",
      "2837/2837 [==============================] - 2s 826us/step - loss: 66.9440 - val_loss: 68.7900 - lr: 0.0010\n",
      "Epoch 21/200\n",
      "2837/2837 [==============================] - 2s 818us/step - loss: 66.6507 - val_loss: 68.6742 - lr: 0.0010\n",
      "Epoch 22/200\n",
      "2837/2837 [==============================] - 2s 842us/step - loss: 66.3048 - val_loss: 69.6044 - lr: 0.0010\n",
      "Epoch 23/200\n",
      "2837/2837 [==============================] - 2s 842us/step - loss: 66.1484 - val_loss: 69.4115 - lr: 0.0010\n",
      "Epoch 24/200\n",
      "2837/2837 [==============================] - 2s 829us/step - loss: 65.7425 - val_loss: 69.4918 - lr: 0.0010\n",
      "Epoch 25/200\n",
      "2837/2837 [==============================] - 2s 829us/step - loss: 65.3914 - val_loss: 68.5292 - lr: 0.0010\n",
      "Epoch 26/200\n",
      "2837/2837 [==============================] - 2s 828us/step - loss: 65.0884 - val_loss: 68.6099 - lr: 0.0010\n",
      "Epoch 27/200\n",
      "2837/2837 [==============================] - 2s 825us/step - loss: 64.7199 - val_loss: 69.0574 - lr: 0.0010\n",
      "Epoch 28/200\n",
      "2837/2837 [==============================] - 2s 833us/step - loss: 64.4540 - val_loss: 69.7801 - lr: 0.0010\n",
      "Epoch 29/200\n",
      "2837/2837 [==============================] - 2s 826us/step - loss: 64.4302 - val_loss: 69.5184 - lr: 0.0010\n",
      "Epoch 30/200\n",
      "2837/2837 [==============================] - 2s 842us/step - loss: 64.0551 - val_loss: 68.9224 - lr: 0.0010\n",
      "Epoch 31/200\n",
      "2837/2837 [==============================] - 2s 834us/step - loss: 61.9679 - val_loss: 67.9231 - lr: 5.0000e-04\n",
      "Epoch 32/200\n",
      "2837/2837 [==============================] - 2s 839us/step - loss: 61.0775 - val_loss: 68.0546 - lr: 5.0000e-04\n",
      "Epoch 33/200\n",
      "2837/2837 [==============================] - 2s 850us/step - loss: 60.7092 - val_loss: 67.7483 - lr: 5.0000e-04\n",
      "Epoch 34/200\n",
      "2837/2837 [==============================] - 2s 847us/step - loss: 60.5146 - val_loss: 67.9089 - lr: 5.0000e-04\n",
      "Epoch 35/200\n",
      "2837/2837 [==============================] - 2s 840us/step - loss: 60.2269 - val_loss: 67.9387 - lr: 5.0000e-04\n",
      "Epoch 36/200\n",
      "2837/2837 [==============================] - 2s 838us/step - loss: 60.1489 - val_loss: 68.5121 - lr: 5.0000e-04\n",
      "Epoch 37/200\n",
      "2837/2837 [==============================] - 3s 887us/step - loss: 59.8094 - val_loss: 67.7008 - lr: 5.0000e-04\n",
      "Epoch 38/200\n",
      "2837/2837 [==============================] - 2s 834us/step - loss: 59.6937 - val_loss: 67.9788 - lr: 5.0000e-04\n",
      "Epoch 39/200\n",
      "2837/2837 [==============================] - 2s 832us/step - loss: 59.5165 - val_loss: 67.8068 - lr: 5.0000e-04\n",
      "Epoch 40/200\n",
      "2837/2837 [==============================] - 2s 840us/step - loss: 59.1277 - val_loss: 68.0769 - lr: 5.0000e-04\n",
      "Epoch 41/200\n",
      "2837/2837 [==============================] - 2s 850us/step - loss: 59.0230 - val_loss: 68.4377 - lr: 5.0000e-04\n",
      "Epoch 42/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 58.9603 - val_loss: 68.1814 - lr: 5.0000e-04\n",
      "Epoch 43/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 57.6175 - val_loss: 67.6927 - lr: 2.5000e-04\n",
      "Epoch 44/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 57.2259 - val_loss: 67.6899 - lr: 2.5000e-04\n",
      "Epoch 45/200\n",
      "2837/2837 [==============================] - 2s 842us/step - loss: 57.0841 - val_loss: 67.6870 - lr: 2.5000e-04\n",
      "Epoch 46/200\n",
      "2837/2837 [==============================] - 2s 842us/step - loss: 56.8501 - val_loss: 67.6762 - lr: 2.5000e-04\n",
      "Epoch 47/200\n",
      "2837/2837 [==============================] - 2s 839us/step - loss: 56.9651 - val_loss: 67.6630 - lr: 2.5000e-04\n",
      "Epoch 48/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 56.8822 - val_loss: 67.8007 - lr: 2.5000e-04\n",
      "Epoch 49/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 56.3731 - val_loss: 67.7746 - lr: 2.5000e-04\n",
      "Epoch 50/200\n",
      "2837/2837 [==============================] - 2s 864us/step - loss: 56.3307 - val_loss: 67.6920 - lr: 2.5000e-04\n",
      "Epoch 51/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 56.2858 - val_loss: 67.8365 - lr: 2.5000e-04\n",
      "Epoch 52/200\n",
      "2837/2837 [==============================] - 2s 833us/step - loss: 56.3174 - val_loss: 67.7873 - lr: 2.5000e-04\n",
      "Epoch 53/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 55.5768 - val_loss: 67.5398 - lr: 1.2500e-04\n",
      "Epoch 54/200\n",
      "2837/2837 [==============================] - 2s 835us/step - loss: 55.2214 - val_loss: 67.7809 - lr: 1.2500e-04\n",
      "Epoch 55/200\n",
      "2837/2837 [==============================] - 2s 822us/step - loss: 55.2386 - val_loss: 67.6513 - lr: 1.2500e-04\n",
      "Epoch 56/200\n",
      "2837/2837 [==============================] - 2s 825us/step - loss: 55.2827 - val_loss: 67.6608 - lr: 1.2500e-04\n",
      "Epoch 57/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 55.0571 - val_loss: 67.6656 - lr: 1.2500e-04\n",
      "Epoch 58/200\n",
      "2837/2837 [==============================] - 2s 832us/step - loss: 55.0863 - val_loss: 67.5225 - lr: 1.2500e-04\n",
      "Epoch 59/200\n",
      "2837/2837 [==============================] - 2s 840us/step - loss: 55.0948 - val_loss: 67.7585 - lr: 1.2500e-04\n",
      "Epoch 60/200\n",
      "2837/2837 [==============================] - 2s 826us/step - loss: 54.8813 - val_loss: 67.6131 - lr: 1.2500e-04\n",
      "Epoch 61/200\n",
      "2837/2837 [==============================] - 2s 837us/step - loss: 54.7323 - val_loss: 67.6638 - lr: 1.2500e-04\n",
      "Epoch 62/200\n",
      "2837/2837 [==============================] - 2s 841us/step - loss: 54.9399 - val_loss: 67.6249 - lr: 1.2500e-04\n",
      "Epoch 63/200\n",
      "2837/2837 [==============================] - 2s 830us/step - loss: 54.5833 - val_loss: 67.9219 - lr: 1.2500e-04\n",
      "Epoch 64/200\n",
      "2837/2837 [==============================] - 2s 840us/step - loss: 54.6098 - val_loss: 67.6420 - lr: 1.0000e-04\n",
      "Epoch 65/200\n",
      "2837/2837 [==============================] - 2s 831us/step - loss: 54.3921 - val_loss: 67.6778 - lr: 1.0000e-04\n",
      "Epoch 66/200\n",
      "2837/2837 [==============================] - 2s 836us/step - loss: 54.5227 - val_loss: 67.6008 - lr: 1.0000e-04\n",
      "Epoch 67/200\n",
      "2837/2837 [==============================] - 2s 833us/step - loss: 54.3616 - val_loss: 67.6113 - lr: 1.0000e-04\n",
      "Epoch 68/200\n",
      "2837/2837 [==============================] - 2s 828us/step - loss: 54.2826 - val_loss: 67.7220 - lr: 1.0000e-04\n",
      "Train Loss migliore epoca: 49.66223907470703\n",
      "Validation Loss migliore epoca: 67.52251434326172\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1))  \n",
    "\n",
    "model.summary()\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Blocca addestramento se non migliora il val_loss per 10 epoche\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Riduci il learning rate se non migliora il val_loss per 5 epoche\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    batch_size=64, \n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "\n",
    "train_loss = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_loss = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"Train Loss migliore epoca: {train_loss}\")\n",
    "print(f\"Validation Loss migliore epoca: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1576/1576 [==============================] - 0s 268us/step\n",
      "MSE: 67.1265814893956\n",
      "RMSE: 8.193081318368298\n",
      "MAE: 5.648528891144026\n",
      "MAPE: 0.00283522677068378\n",
      "R2: 0.3841174265275764\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MAPE:\", mean_absolute_percentage_error(y_test, y_pred))\n",
    "print(\"R2:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-05 09:24:38.441149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,256]\n",
      "\t [[{{node inputs}}]]\n",
      "2024-02-05 09:24:38.585232: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [?,256]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelli/NN/model/assets\n"
     ]
    }
   ],
   "source": [
    "file_scaler = open(\"modelli/NN/scaler_nn.save\",\"wb\")\n",
    "pickle.dump(scaler, file_scaler)\n",
    "file_scaler.close()\n",
    "model.save(\"modelli/NN/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
